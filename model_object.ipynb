{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb8880d-d381-4f49-ae81-9ea7fd19f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095a8a2c-a385-42d2-b87e-4dac672320a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import sine_data\n",
    "X, y = sine_data()\n",
    "\n",
    "# transpose the matrices\n",
    "X = X.T\n",
    "y = y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6858e2cd-887d-4c53-9eb7-c31640b14a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape =  (1, 1000)\n",
      "y shape =  (1, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape = \", X.shape)\n",
    "print(\"y shape = \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b47722f-91f3-4511-b1e7-ea6bc85be45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NN_ClassModel import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8cb638-5a06-4930-a6e5-f3dfb7d8b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons_h = 64\n",
    "n_neurons_o = 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "563c1942-f548-410c-84b4-ab8bcfc8a2d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Instantiate the model\n",
    "model = Model()\n",
    "\n",
    "# Add layers\n",
    "model.add(Layer_Dense( 64 , 1 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 64 , 64 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 1 , 64 ))\n",
    "model.add(Activation_Linear())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "loss = Loss_MeanSquaredError(),\n",
    "optimizer = Optimizer_Adam(learning_rate = 0.005 , decay = 1e-3),\n",
    "accuracy = Accuracy_Regression()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs = 10000 , print_every = 100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d19f80a-99c9-4eb5-954e-7e0add55c792",
   "metadata": {},
   "source": [
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'\n",
    "\n",
    "if not os.path.isfile(FILE):\n",
    "    print (f'Downloading {URL} and saving as {FILE} ...')\n",
    "    urllib.request.urlretrieve(URL, FILE)\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "print ( 'Unzipping images...' )\n",
    "with ZipFile(FILE) as zip_images:\n",
    "    zip_images.extractall(FOLDER)\n",
    "print ( 'Done!' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f7bb3f-a74b-464a-ac25-983919e3bcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the image data:\n",
    "import cv2\n",
    "\n",
    "image_data = cv2.imread('fashion_mnist_images/train/7/0002.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84861037-7a3e-4ef1-ab3c-e549d5f8497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan all the directories and create a list of labels\n",
    "labels = os.listdir('fashion_mnist_images/train')\n",
    "\n",
    "# Loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset , path):\n",
    "    \n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "\n",
    "    # Create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        \n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "        \n",
    "            # Read the image\n",
    "            image = cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9237d39-9776-4d34-a3fa-29e6924016c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset (train + test)\n",
    "def create_data_mnist(path):\n",
    "    \n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train' , path)\n",
    "    X_test, y_test = load_mnist_dataset('test' , path)\n",
    "    \n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c52eacd-d98b-423c-a173-a3df0087a916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of X : (60000, 28, 28)\n",
      "the shape of y : (60000,)\n",
      "the shape of X_test : (10000, 28, 28)\n",
      "the shape of y_test : (10000,)\n"
     ]
    }
   ],
   "source": [
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "\n",
    "print('the shape of X :', X.shape)\n",
    "print('the shape of y :', y.shape)\n",
    "print('the shape of X_test :', X_test.shape)\n",
    "print('the shape of y_test :', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a967ed95-2792-4498-a454-6740fb52f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "# \n",
    "# Scale and reshape samples\n",
    "X = X/255\n",
    "X_test = X_test/255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825b5cb1-cbea-447b-9a0d-d862dd46995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], -1)\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "X = X.T\n",
    "y = y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa6dbb82-61bf-4305-bf45-b1b772f0c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "y_test = y_test.reshape(y_test.shape[0], 1)\n",
    "X_test = X_test.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255cf2ec-95bb-48d6-8f04-acc29ca8734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of X : (784, 60000)\n",
      "the shape of y : (1, 60000)\n",
      "the shape of X_test : (784, 10000)\n",
      "the shape of y_test : (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "print('the shape of X :', X.shape)\n",
    "print('the shape of y :', y.shape)\n",
    "print('the shape of X_test :', X_test.shape)\n",
    "print('the shape of y_test :', y_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03b4387a-3452-4e76-b5a8-0eaf526b04bf",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3)\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data( samples = 100 , classes = 3)\n",
    "X = X.T\n",
    "y = y.T\n",
    "X_test = X_test.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0694943-b56a-4468-bfe9-0376f128e9d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===] step: 0- loss: 2.303 (data_loss: 2.3005, reg loss: 0.002) - accuracy: 0.141 - lr: 0.001\n",
      "[===] step: 100- loss: 0.8639 (data_loss: 0.8291, reg loss: 0.035) - accuracy: 0.672 - lr: 0.000995\n",
      "[===] step: 200- loss: 0.6419 (data_loss: 0.5968, reg loss: 0.045) - accuracy: 0.82 - lr: 0.00099\n",
      "[===] step: 300- loss: 0.6354 (data_loss: 0.5854, reg loss: 0.05) - accuracy: 0.773 - lr: 0.000985\n",
      "[===] step: 400- loss: 0.6406 (data_loss: 0.5882, reg loss: 0.052) - accuracy: 0.828 - lr: 0.00098\n",
      "\t✅: [██████████] 466/468 steps done\n",
      "\t✅: [██████████] 467/468 steps done\n",
      "[===] step: 468- loss: 0.5755 (data_loss: 0.5221, reg loss: 0.053) - accuracy: 0.833 - lr: 0.000977\n",
      "\t✅: [██████████] 468/468 steps done\n",
      "Epoch 1/5\n",
      "[=====Training] - loss: 0.7819 (data_loss: 0.7284, reg loss: 0.053) - accuracy: 0.75 - lr: 0.000977\n",
      "***** Validation:  - validation_loss: 0.5281 - validation_accuracy: 0.817\n",
      "\n",
      "\n",
      "[===] step: 0- loss: 0.5707 (data_loss: 0.5173, reg loss: 0.053) - accuracy: 0.82 - lr: 0.000977\n",
      "[===] step: 100- loss: 0.5586 (data_loss: 0.5043, reg loss: 0.054) - accuracy: 0.82 - lr: 0.000972\n",
      "[===] step: 200- loss: 0.5617 (data_loss: 0.5074, reg loss: 0.054) - accuracy: 0.82 - lr: 0.000968\n",
      "[===] step: 300- loss: 0.54 (data_loss: 0.4862, reg loss: 0.054) - accuracy: 0.805 - lr: 0.000963\n",
      "[===] step: 400- loss: 0.568 (data_loss: 0.5138, reg loss: 0.054) - accuracy: 0.859 - lr: 0.000958\n",
      "\t✅: [██████████] 466/468 steps done\n",
      "\t✅: [██████████] 467/468 steps done\n",
      "[===] step: 468- loss: 0.5227 (data_loss: 0.4686, reg loss: 0.054) - accuracy: 0.875 - lr: 0.000955\n",
      "\t✅: [██████████] 468/468 steps done\n",
      "Epoch 2/5\n",
      "[=====Training] - loss: 0.5258 (data_loss: 0.4716, reg loss: 0.054) - accuracy: 0.839 - lr: 0.000955\n",
      "***** Validation:  - validation_loss: 0.4751 - validation_accuracy: 0.833\n",
      "\n",
      "\n",
      "[===] step: 0- loss: 0.4958 (data_loss: 0.4417, reg loss: 0.054) - accuracy: 0.828 - lr: 0.000955\n",
      "[===] step: 100- loss: 0.5206 (data_loss: 0.4668, reg loss: 0.054) - accuracy: 0.836 - lr: 0.000951\n",
      "[===] step: 200- loss: 0.5248 (data_loss: 0.4714, reg loss: 0.053) - accuracy: 0.844 - lr: 0.000946\n",
      "[===] step: 300- loss: 0.5004 (data_loss: 0.4477, reg loss: 0.053) - accuracy: 0.828 - lr: 0.000942\n",
      "[===] step: 400- loss: 0.5343 (data_loss: 0.4815, reg loss: 0.053) - accuracy: 0.875 - lr: 0.000937\n",
      "\t✅: [██████████] 466/468 steps done\n",
      "\t✅: [██████████] 467/468 steps done\n",
      "[===] step: 468- loss: 0.5038 (data_loss: 0.4511, reg loss: 0.053) - accuracy: 0.896 - lr: 0.000934\n",
      "\t✅: [██████████] 468/468 steps done\n",
      "Epoch 3/5\n",
      "[=====Training] - loss: 0.4843 (data_loss: 0.4316, reg loss: 0.053) - accuracy: 0.851 - lr: 0.000934\n",
      "***** Validation:  - validation_loss: 0.4509 - validation_accuracy: 0.84\n",
      "\n",
      "\n",
      "[===] step: 0- loss: 0.471 (data_loss: 0.4183, reg loss: 0.053) - accuracy: 0.828 - lr: 0.000934\n",
      "[===] step: 100- loss: 0.5007 (data_loss: 0.4485, reg loss: 0.052) - accuracy: 0.852 - lr: 0.00093\n",
      "[===] step: 200- loss: 0.5 (data_loss: 0.4483, reg loss: 0.052) - accuracy: 0.844 - lr: 0.000926\n",
      "[===] step: 300- loss: 0.4795 (data_loss: 0.4285, reg loss: 0.051) - accuracy: 0.828 - lr: 0.000921\n",
      "[===] step: 400- loss: 0.5141 (data_loss: 0.4629, reg loss: 0.051) - accuracy: 0.867 - lr: 0.000917\n",
      "\t✅: [██████████] 466/468 steps done\n",
      "\t✅: [██████████] 467/468 steps done\n",
      "[===] step: 468- loss: 0.4915 (data_loss: 0.4403, reg loss: 0.051) - accuracy: 0.896 - lr: 0.000914\n",
      "\t✅: [██████████] 468/468 steps done\n",
      "Epoch 4/5\n",
      "[=====Training] - loss: 0.4624 (data_loss: 0.4112, reg loss: 0.051) - accuracy: 0.858 - lr: 0.000914\n",
      "***** Validation:  - validation_loss: 0.4393 - validation_accuracy: 0.846\n",
      "\n",
      "\n",
      "[===] step: 0- loss: 0.4624 (data_loss: 0.4112, reg loss: 0.051) - accuracy: 0.82 - lr: 0.000914\n",
      "[===] step: 100- loss: 0.476 (data_loss: 0.4254, reg loss: 0.051) - accuracy: 0.859 - lr: 0.00091\n",
      "[===] step: 200- loss: 0.4845 (data_loss: 0.4341, reg loss: 0.05) - accuracy: 0.859 - lr: 0.000906\n",
      "[===] step: 300- loss: 0.4659 (data_loss: 0.4162, reg loss: 0.05) - accuracy: 0.844 - lr: 0.000902\n",
      "[===] step: 400- loss: 0.4966 (data_loss: 0.4467, reg loss: 0.05) - accuracy: 0.852 - lr: 0.000898\n",
      "\t✅: [██████████] 466/468 steps done\n",
      "\t✅: [██████████] 467/468 steps done\n",
      "[===] step: 468- loss: 0.4833 (data_loss: 0.4333, reg loss: 0.05) - accuracy: 0.896 - lr: 0.000895\n",
      "\t✅: [██████████] 468/468 steps done\n",
      "Epoch 5/5\n",
      "[=====Training] - loss: 0.4468 (data_loss: 0.3968, reg loss: 0.05) - accuracy: 0.862 - lr: 0.000895\n",
      "***** Validation:  - validation_loss: 0.4324 - validation_accuracy: 0.849\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Model()\n",
    "\n",
    "# Add layers\n",
    "model.add(Layer_Dense(64, X.shape[0], weight_regularizer_L2 = 5e-4 ,bias_regularizer_L2 = 5e-4))\n",
    "model.add(Activation_ReLU())\n",
    "# model.add(Layer_Dense( 64 , 64 ))\n",
    "# model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(10 , 64 ))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "loss = Loss_CategoricalCrossentropy(),\n",
    "optimizer = Optimizer_Adam( learning_rate = 0.001 , decay = 5e-5),\n",
    "accuracy = Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs = 5, batch_size = 128, print_every = 100, validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
